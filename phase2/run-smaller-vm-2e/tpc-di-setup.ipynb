{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90a0ed7a-a23a-4523-9171-f1d076700168",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_BUCKET=tbd-2024z-303772-data\n",
      "env: GEN_OUTPUT_DIR=/tmp/tpc-di\n",
      "env: REPO_ROOT=/home/jupyter/git/tbd-tpc-di/\n"
     ]
    }
   ],
   "source": [
    "%env DATA_BUCKET=tbd-2024z-303772-data\n",
    "%env GEN_OUTPUT_DIR=/tmp/tpc-di\n",
    "%env REPO_ROOT=/home/jupyter/git/tbd-tpc-di/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fbec0d-d410-4de9-8dfd-00b07fdb7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting typer==0.9.0 (from typer[All]==0.9.0)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting google-cloud-storage==2.13.0\n",
      "  Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from typer==0.9.0->typer[All]==0.9.0) (4.12.2)\n",
      "Collecting google-auth<3.0dev,>=2.23.3 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage==2.13.0) (2.32.3)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage==2.13.0)\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from typer[All]==0.9.0) (0.4.6)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[All]==0.9.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[All]==0.9.0)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0) (4.25.5)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage==2.13.0)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading cachetools-5.5.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage==2.13.0) (2019.11.28)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from rich<14.0.0,>=10.11.0->typer[All]==0.9.0) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[All]==0.9.0)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage==2.13.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading cachetools-5.5.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: typer, shellingham, pyasn1, proto-plus, mdurl, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, markdown-it-py, google-resumable-media, rich, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.5.1 google-api-core-2.24.0 google-auth-2.38.0 google-cloud-core-2.4.1 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.7.2 googleapis-common-protos-1.66.0 markdown-it-py-3.0.0 mdurl-0.1.2 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rich-13.9.4 rsa-4.9 shellingham-1.5.4 typer-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3.8 install typer[All]==0.9.0 google-cloud-storage==2.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b5820-852d-4c30-a178-9dd78bbdbd5f",
   "metadata": {},
   "source": [
    "## Install SDKMAN for setting up JVM 8 enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3cdb5b-4978-479e-85c7-c9dbac3e65fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                -+syyyyyyys:\n",
      "                            `/yho:`       -yd.\n",
      "                         `/yh/`             +m.\n",
      "                       .oho.                 hy                          .`\n",
      "                     .sh/`                   :N`                `-/o`  `+dyyo:.\n",
      "                   .yh:`                     `M-          `-/osysoym  :hs` `-+sys:      hhyssssssssy+\n",
      "                 .sh:`                       `N:          ms/-``  yy.yh-      -hy.    `.N-````````+N.\n",
      "               `od/`                         `N-       -/oM-      ddd+`     `sd:     hNNm        -N:\n",
      "              :do`                           .M.       dMMM-     `ms.      /d+`     `NMMs       `do\n",
      "            .yy-                             :N`    ```mMMM.      -      -hy.       /MMM:       yh\n",
      "          `+d+`           `:/oo/`       `-/osyh/ossssssdNMM`           .sh:         yMMN`      /m.\n",
      "         -dh-           :ymNMMMMy  `-/shmNm-`:N/-.``   `.sN            /N-         `NMMy      .m/\n",
      "       `oNs`          -hysosmMMMMydmNmds+-.:ohm           :             sd`        :MMM/      yy\n",
      "      .hN+           /d:    -MMMmhs/-.`   .MMMh   .ss+-                 `yy`       sMMN`     :N.\n",
      "     :mN/           `N/     `o/-`         :MMMo   +MMMN-         .`      `ds       mMMh      do\n",
      "    /NN/            `N+....--:/+oooosooo+:sMMM:   hMMMM:        `my       .m+     -MMM+     :N.\n",
      "   /NMo              -+ooooo+/:-....`...:+hNMN.  `NMMMd`        .MM/       -m:    oMMN.     hs\n",
      "  -NMd`                                    :mm   -MMMm- .s/     -MMm.       /m-   mMMd     -N.\n",
      " `mMM/                                      .-   /MMh. -dMo     -MMMy        od. .MMMs..---yh\n",
      " +MMM.                                           sNo`.sNMM+     :MMMM/        sh`+MMMNmNm+++-\n",
      " mMMM-                                           /--ohmMMM+     :MMMMm.       `hyymmmdddo\n",
      " MMMMh.                  ````                  `-+yy/`yMMM/     :MMMMMy       -sm:.``..-:-.`\n",
      " dMMMMmo-.``````..-:/osyhddddho.           `+shdh+.   hMMM:     :MmMMMM/   ./yy/` `:sys+/+sh/\n",
      " .dMMMMMMmdddddmmNMMMNNNNNMMMMMs           sNdo-      dMMM-  `-/yd/MMMMm-:sy+.   :hs-      /N`\n",
      "  `/ymNNNNNNNmmdys+/::----/dMMm:          +m-         mMMM+ohmo/.` sMMMMdo-    .om:       `sh\n",
      "     `.-----+/.`       `.-+hh/`         `od.          NMMNmds/     `mmy:`     +mMy      `:yy.\n",
      "           /moyso+//+ossso:.           .yy`          `dy+:`         ..       :MMMN+---/oys:\n",
      "         /+m:  `.-:::-`               /d+                                    +MMMMMMMNh:`\n",
      "        +MN/                        -yh.                                     `+hddhy+.\n",
      "       /MM+                       .sh:\n",
      "      :NMo                      -sh/\n",
      "     -NMs                    `/yy:\n",
      "    .NMy                  `:sh+.\n",
      "   `mMm`               ./yds-\n",
      "  `dMMMmyo:-.````.-:oymNy:`\n",
      "  +NMMMMMMMMMMMMMMMMms:`\n",
      "    -+shmNMMMNmdy+:`\n",
      "\n",
      "\n",
      "                                                                 Now attempting installation...\n",
      "\n",
      "\n",
      "Looking for a previous installation of SDKMAN...\n",
      "Looking for unzip...\n",
      "Looking for zip...\n",
      "Looking for tar...\n",
      "Looking for curl...\n",
      "Looking for sed...\n",
      "Installing SDKMAN scripts...\n",
      "Create distribution directories...\n",
      "Getting available candidates...\n",
      "Prime platform file...\n",
      "Prime the config file...\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Installing script cli archive...\n",
      "* Downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Checking archive integrity...\n",
      "* Extracting archive...\n",
      "* Copying archive contents...\n",
      "* Cleaning up...\n",
      "\n",
      "Set version to 5.19.0 ...\n",
      "Set native version to 0.5.0 ...\n",
      "Attempt update of interactive bash profile on regular UNIX...\n",
      "Added sdkman init snippet to /root/.bashrc\n",
      "Attempt update of zsh profile...\n",
      "Updated existing /root/.zshrc\n",
      "\n",
      "\n",
      "\n",
      "All done!\n",
      "\n",
      "\n",
      "You are subscribed to the STABLE channel.\n",
      "\n",
      "Please open a new terminal, or run the following in the existing one:\n",
      "\n",
      "    source \"/root/.sdkman/bin/sdkman-init.sh\"\n",
      "\n",
      "Then issue the following command:\n",
      "\n",
      "    sdk help\n",
      "\n",
      "Enjoy!!!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -s https://get.sdkman.io | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691084f8-a883-408a-8d16-cc717669c04d",
   "metadata": {},
   "source": [
    "## Install and set as default JVM 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275488e6-68e3-4f42-a3ca-060b286bade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 8.0.392-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 8.0.392-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 8.0.392-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;32mSetting java 8.0.392-amzn as default.\u001b[0m\n",
      "\n",
      "\u001b[1;32mUsing java version 8.0.392-amzn in this shell.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 8.0.392-amzn\n",
    "sdk use java 8.0.392-amzn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554db58-b832-45e2-a1f6-28bd873ae31d",
   "metadata": {},
   "source": [
    "## Check if JVM 8 is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898835b7-c347-49dd-bc0f-ca845375e1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45d9a3-e599-4661-944b-1bdfe3808c19",
   "metadata": {},
   "source": [
    "## Clone tbd-tpc-di repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7481d445-4ea6-40cc-8320-5520fe8d4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'tbd-tpc-di'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Switched to a new branch 'notebook'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'notebook' set up to track remote branch 'notebook' from 'origin'.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p git && cd git\n",
    "git clone https://github.com/karolstepanienko/tbd-tpc-di.git\n",
    "cd tbd-tpc-di\n",
    "git pull\n",
    "git checkout notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992bfc-8f8c-42eb-bf07-069b0d897fda",
   "metadata": {},
   "source": [
    "## Generate input dataset (run this cell below from the terminal!!!)\n",
    "It should take approx. 15min with scale factor set to 100 and generate approx. 10GiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "790ab901-15dc-4e94-b4fd-e2b9cfc6e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment Corretto-8.392.08.1 (build 1.8.0_392-b08)\n",
      "OpenJDK 64-Bit Server VM Corretto-8.392.08.1 (build 25.392-b08, mixed mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/git/tbd-tpc-di/tools/./output\n",
      "########################################################################################################################\n",
      "                                                  PDGF v2.5_#1343_b4177\n",
      "                                            Parallel Data Generation Framework\n",
      "                (c)bankmark UG (haftungsbeschraenkt), Frank M., Danisch M., Rabl T. http://www.bankmark.de\n",
      "########################################################################################################################\n",
      "                                                   License information\n",
      "                            The Software is provided to you as part of the TPC Benchmark DI. \n",
      " When using this software you must agree to the license provided in LICENSE.TXT of this package. Use is restricted to TPC\n",
      "DI benchmarking purposes as specified in LICENSE.TXT. If you would like to use the software for other purposes, you must\n",
      "contact bankmark UG (haftungsbeschraenkt) (http://www.bankmark.de) to purchase a fully licensed copy of the Software.\n",
      "########################################################################################################################\n",
      "for a command overview start with commandline parameter: -help\n",
      "or type \"help\" in the built in shell: PDGF:> \n",
      "\n",
      "Set closeWhenDone from: false to: true\n",
      "Set project main scale factor from -notSet- to 10000.0\n",
      "Set default output path to:\"/home/jupyter/git/tbd-tpc-di/tools/./output/\"\n",
      "Loading configuration files ...\n",
      "23: XML <property name=\"SF\">5000.0 was already defined in config file or is manually overridden by a user set commandline argument. Currently assigned value: '10000.0' and will remain unchanged at this value\n",
      "Registered Generation event listner: tpc.di.output.AuditTotalRecordsSummaryWriter\n",
      "Loading successful.\n",
      "All required configuration files are loaded. You may start the generation process.\n",
      "Initializing system...\n",
      "Node 1 Worker count was not specified. Detected 2 available processors and start now same amount of workers\n",
      "FinWireFinBlackBox.initialize() LAST UPDATE ID: 202\n",
      "DailyMarketBlackBox.initialize() LAST UPDATE ID: 733\n",
      "initializing WatchHistoryBlackBox of table: 'WatchHistory'... done\n",
      "Cloning data structures for parallelization...\n",
      "Clone time 0h:00m:00s:207ms\n",
      "initialized 0h:00m:01s:082ms\n",
      "Starting data generation proccess...\n",
      "Startuptime: 0h:00m:03s:391ms\n",
      "FileChannelProvider is: pdgf.util.caching.fileWriter.OutputFileWriter$DefaultFileChannelProvider\n",
      "\n",
      "generating 1/16 \"StatusType\"\t  rows: 1-6 of 6\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n",
      "PDGF:> ERROR! Command 'null' is unknown. Enter 'help' to see a list of available commands.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR!  Flooding prevention! to much failed commands in a row...terminating application\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDGF:> Stopping data generation proccess\n",
      "Data generation aborted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.io.IOException: Broken pipe\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:297)\n",
      "\tat sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:141)\n",
      "\tat java.io.OutputStreamWriter.flush(OutputStreamWriter.java:229)\n",
      "\tat org.tpc.di.digen.DIGen$3.run(DIGen.java:242)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIGen failed with non-zero return code from PDGF.\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd /home/jupyter/git/tbd-tpc-di/tools/\n",
    "java -version\n",
    "java -jar DIGen.jar -sf 100 -o /tmp/tpc-di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a24c0aa-bff7-4831-869d-3183e7373a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup JVM 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e466e8-41aa-4afc-8392-5675d36adcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: java 11.0.21-amzn\n",
      "\n",
      "In progress...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "######################################################################## 100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repackaging Java 11.0.21-amzn...\n",
      "\n",
      "Done repackaging...\n",
      "\n",
      "\u001b[1;32mInstalling: java 11.0.21-amzn\u001b[0m\n",
      "\u001b[1;32mDone installing!\u001b[0m\n",
      "\n",
      "\u001b[1;33mDo you want java 11.0.21-amzn to be set as default? (Y/n): \u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java 11.0.21-amzn\n",
    "sdk use java 11.0.21-amzn -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dccda4c-40d4-40fe-9d2e-68b416522f64",
   "metadata": {},
   "source": [
    "## Load staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8de588d-c967-4480-9eb3-d74a64f0f0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.25\" 2024-10-15\n",
      "OpenJDK Runtime Environment (build 11.0.25+9-post-Ubuntu-1ubuntu120.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.25+9-post-Ubuntu-1ubuntu120.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "259b72bc-2efd-405b-883f-618a9772bf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e78f179-8511-4753-926e-f34f5375e97e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.17.0/spark-xml_2.12-0.17.0.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#spark-xml_2.12;0.17.0!spark-xml_2.12.jar (36ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.11.0!commons-io.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/jaxb/txw2/3.0.2/txw2-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish.jaxb#txw2;3.0.2!txw2.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/ws/xmlschema/xmlschema-core/2.3.0/xmlschema-core-2.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.ws.xmlschema#xmlschema-core;2.3.0!xmlschema-core.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.9.0/scala-collection-compat_2.12-2.9.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.9.0!scala-collection-compat_2.12.jar (41ms)\n",
      ":: resolution report :: resolve 6368ms :: artifacts dl 235ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9e78f179-8511-4753-926e-f34f5375e97e\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (989kB/27ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 13:25:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 13:25:28 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/01/24 13:25:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/24 13:25:40 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:25:40 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:25:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/01/24 13:25:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:25:40 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:26:03 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "25/01/24 13:31:09 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "DATE table created.\n",
      "DAILY_MARKET table created.\n",
      "INDUSTRY table created.\n",
      "PROSPECT table created.\n",
      "CUSTOMER_MGMT table created.\n",
      "TAX_RATE table created.\n",
      "HR table created.\n",
      "WATCH_HISTORY table created.\n",
      "TRADE table created.\n",
      "TRADE_HISTORY table created.\n",
      "STATUS_TYPE table created.\n",
      "TRADE_TYPE table created.\n",
      "HOLDING_HISTORY table created.\n",
      "CASH_TRANSACTION table created.\n",
      "CMP table created.\n",
      "SEC table created.\n",
      "FIN table created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "cd $REPO_ROOT\n",
    "python3.8 tpcdi.py --output-directory $GEN_OUTPUT_DIR --stage $DATA_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0740193-5da4-4aac-9349-aa4e76f4e99b",
   "metadata": {},
   "source": [
    "## Run dbt ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98d0642c-b5c9-4aa8-996f-40f82e3c90e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m13:47:17  Running with dbt=1.7.13\n",
      "\u001b[0m13:47:17  Updating lock file in file path: /home/jupyter/git/tbd-tpc-di/package-lock.yml\n",
      "\u001b[0m13:47:17  Installing dbt-labs/dbt_utils\n",
      "\u001b[0m13:47:18  Installed from version 1.1.1\n",
      "\u001b[0m13:47:18  Updated version available: 1.3.0\n",
      "\u001b[0m13:47:18  \n",
      "\u001b[0m13:47:18  Updates available for packages: ['dbt-labs/dbt_utils']                 \n",
      "Update your versions in packages.yml, then run dbt deps\n",
      "\u001b[0m13:47:21  Running with dbt=1.7.13\n",
      "\u001b[0m13:47:22  Registered adapter: spark=1.7.1\n",
      "\u001b[0m13:47:22  Unable to do partial parsing because saved manifest not found. Starting full parse.\n",
      "\u001b[0m13:47:24  Found 44 models, 4 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m13:47:24  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-768b4ca8-e227-4fe5-87fe-a1654a64d7ac;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 750ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-768b4ca8-e227-4fe5-87fe-a1654a64d7ac\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/16ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 13:47:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 13:47:34 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/01/24 13:47:35 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/24 13:47:51 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:47:51 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:47:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/01/24 13:47:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:47:51 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/01/24 13:48:09 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m13:48:11  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m13:48:11  \n",
      "\u001b[0m13:48:11  1 of 43 START sql table model demo_bronze.brokerage_cash_transaction ........... [RUN]\n",
      "25/01/24 13:48:13 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/01/24 13:48:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m13:48:51  1 of 43 OK created sql table model demo_bronze.brokerage_cash_transaction ...... [\u001b[32mOK\u001b[0m in 39.17s]\n",
      "\u001b[0m13:48:51  2 of 43 START sql table model demo_bronze.brokerage_daily_market ............... [RUN]\n",
      "25/01/24 13:48:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:50:26  2 of 43 OK created sql table model demo_bronze.brokerage_daily_market .......... [\u001b[32mOK\u001b[0m in 94.98s]\n",
      "\u001b[0m13:50:26  3 of 43 START sql table model demo_bronze.brokerage_holding_history ............ [RUN]\n",
      "25/01/24 13:50:26 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:50:39  3 of 43 OK created sql table model demo_bronze.brokerage_holding_history ....... [\u001b[32mOK\u001b[0m in 13.75s]\n",
      "\u001b[0m13:50:39  4 of 43 START sql table model demo_bronze.brokerage_trade ...................... [RUN]\n",
      "25/01/24 13:50:40 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:51:33  4 of 43 OK created sql table model demo_bronze.brokerage_trade ................. [\u001b[32mOK\u001b[0m in 53.94s]\n",
      "\u001b[0m13:51:33  5 of 43 START sql table model demo_bronze.brokerage_trade_history .............. [RUN]\n",
      "25/01/24 13:51:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:52:19  5 of 43 OK created sql table model demo_bronze.brokerage_trade_history ......... [\u001b[32mOK\u001b[0m in 45.26s]\n",
      "\u001b[0m13:52:19  6 of 43 START sql table model demo_bronze.brokerage_watch_history .............. [RUN]\n",
      "25/01/24 13:52:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:53:11  6 of 43 OK created sql table model demo_bronze.brokerage_watch_history ......... [\u001b[32mOK\u001b[0m in 52.01s]\n",
      "\u001b[0m13:53:11  7 of 43 START sql table model demo_bronze.crm_customer_mgmt .................... [RUN]\n",
      "25/01/24 13:53:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/01/24 13:53:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "\u001b[0m13:53:20  7 of 43 OK created sql table model demo_bronze.crm_customer_mgmt ............... [\u001b[32mOK\u001b[0m in 9.59s]\n",
      "\u001b[0m13:53:20  8 of 43 START sql table model demo_bronze.finwire_company ...................... [RUN]\n",
      "25/01/24 13:53:20 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:53:23  8 of 43 OK created sql table model demo_bronze.finwire_company ................. [\u001b[32mOK\u001b[0m in 2.29s]\n",
      "\u001b[0m13:53:23  9 of 43 START sql table model demo_bronze.finwire_financial .................... [RUN]\n",
      "25/01/24 13:53:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:18  9 of 43 OK created sql table model demo_bronze.finwire_financial ............... [\u001b[32mOK\u001b[0m in 55.80s]\n",
      "\u001b[0m13:54:18  10 of 43 START sql table model demo_bronze.finwire_security .................... [RUN]\n",
      "25/01/24 13:54:18 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:21  10 of 43 OK created sql table model demo_bronze.finwire_security ............... [\u001b[32mOK\u001b[0m in 2.67s]\n",
      "\u001b[0m13:54:21  11 of 43 START sql table model demo_bronze.hr_employee ......................... [RUN]\n",
      "25/01/24 13:54:21 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:24  11 of 43 OK created sql table model demo_bronze.hr_employee .................... [\u001b[32mOK\u001b[0m in 3.03s]\n",
      "\u001b[0m13:54:24  12 of 43 START sql table model demo_bronze.reference_date ...................... [RUN]\n",
      "25/01/24 13:54:24 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:25  12 of 43 OK created sql table model demo_bronze.reference_date ................. [\u001b[32mOK\u001b[0m in 1.29s]\n",
      "\u001b[0m13:54:25  13 of 43 START sql table model demo_bronze.reference_industry .................. [RUN]\n",
      "25/01/24 13:54:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:26  13 of 43 OK created sql table model demo_bronze.reference_industry ............. [\u001b[32mOK\u001b[0m in 0.77s]\n",
      "\u001b[0m13:54:26  14 of 43 START sql table model demo_bronze.reference_status_type ............... [RUN]\n",
      "25/01/24 13:54:26 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:27  14 of 43 OK created sql table model demo_bronze.reference_status_type .......... [\u001b[32mOK\u001b[0m in 0.88s]\n",
      "\u001b[0m13:54:27  15 of 43 START sql table model demo_bronze.reference_tax_rate .................. [RUN]\n",
      "25/01/24 13:54:27 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:28  15 of 43 OK created sql table model demo_bronze.reference_tax_rate ............. [\u001b[32mOK\u001b[0m in 0.87s]\n",
      "\u001b[0m13:54:28  16 of 43 START sql table model demo_bronze.reference_trade_type ................ [RUN]\n",
      "25/01/24 13:54:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:29  16 of 43 OK created sql table model demo_bronze.reference_trade_type ........... [\u001b[32mOK\u001b[0m in 1.00s]\n",
      "\u001b[0m13:54:29  17 of 43 START sql table model demo_bronze.syndicated_prospect ................. [RUN]\n",
      "25/01/24 13:54:29 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m13:54:35  17 of 43 OK created sql table model demo_bronze.syndicated_prospect ............ [\u001b[32mOK\u001b[0m in 5.80s]\n",
      "\u001b[0m13:54:35  18 of 43 START sql table model demo_silver.daily_market ........................ [RUN]\n",
      "25/01/24 13:54:35 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:22  18 of 43 OK created sql table model demo_silver.daily_market ................... [\u001b[32mOK\u001b[0m in 1187.70s]\n",
      "\u001b[0m14:14:22  19 of 43 START sql table model demo_silver.employees ........................... [RUN]\n",
      "25/01/24 14:14:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:26  19 of 43 OK created sql table model demo_silver.employees ...................... [\u001b[32mOK\u001b[0m in 3.46s]\n",
      "\u001b[0m14:14:26  20 of 43 START sql table model demo_silver.date ................................ [RUN]\n",
      "25/01/24 14:14:26 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:27  20 of 43 OK created sql table model demo_silver.date ........................... [\u001b[32mOK\u001b[0m in 1.42s]\n",
      "\u001b[0m14:14:27  21 of 43 START sql table model demo_silver.companies ........................... [RUN]\n",
      "25/01/24 14:14:28 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:33  21 of 43 OK created sql table model demo_silver.companies ...................... [\u001b[32mOK\u001b[0m in 6.14s]\n",
      "\u001b[0m14:14:33  22 of 43 START sql table model demo_silver.accounts ............................ [RUN]\n",
      "25/01/24 14:14:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:46  22 of 43 OK created sql table model demo_silver.accounts ....................... [\u001b[32mOK\u001b[0m in 12.42s]\n",
      "\u001b[0m14:14:46  23 of 43 START sql table model demo_silver.customers ........................... [RUN]\n",
      "25/01/24 14:14:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:14:56  23 of 43 OK created sql table model demo_silver.customers ...................... [\u001b[32mOK\u001b[0m in 9.29s]\n",
      "\u001b[0m14:14:56  24 of 43 START sql table model demo_silver.trades_history ...................... [RUN]\n",
      "25/01/24 14:14:56 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:19:58  24 of 43 OK created sql table model demo_silver.trades_history ................. [\u001b[32mOK\u001b[0m in 302.61s]\n",
      "\u001b[0m14:19:58  25 of 43 START sql table model demo_gold.dim_broker ............................ [RUN]\n",
      "25/01/24 14:19:58 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:20:05  25 of 43 OK created sql table model demo_gold.dim_broker ....................... [\u001b[32mOK\u001b[0m in 6.20s]\n",
      "\u001b[0m14:20:05  26 of 43 START sql table model demo_gold.dim_date .............................. [RUN]\n",
      "25/01/24 14:20:05 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:20:07  26 of 43 OK created sql table model demo_gold.dim_date ......................... [\u001b[32mOK\u001b[0m in 1.93s]\n",
      "\u001b[0m14:20:07  27 of 43 START sql table model demo_gold.dim_company ........................... [RUN]\n",
      "25/01/24 14:20:07 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:20:10  27 of 43 OK created sql table model demo_gold.dim_company ...................... [\u001b[32mOK\u001b[0m in 3.50s]\n",
      "\u001b[0m14:20:10  28 of 43 START sql table model demo_silver.financials .......................... [RUN]\n",
      "25/01/24 14:20:10 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:21:17  28 of 43 OK created sql table model demo_silver.financials ..................... [\u001b[32mOK\u001b[0m in 67.13s]\n",
      "\u001b[0m14:21:17  29 of 43 START sql table model demo_silver.securities .......................... [RUN]\n",
      "25/01/24 14:21:17 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:21:23  29 of 43 OK created sql table model demo_silver.securities ..................... [\u001b[32mOK\u001b[0m in 5.45s]\n",
      "\u001b[0m14:21:23  30 of 43 START sql table model demo_silver.cash_transactions ................... [RUN]\n",
      "25/01/24 14:21:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:22:08  30 of 43 OK created sql table model demo_silver.cash_transactions .............. [\u001b[32mOK\u001b[0m in 45.70s]\n",
      "\u001b[0m14:22:08  31 of 43 START sql table model demo_gold.dim_customer .......................... [RUN]\n",
      "25/01/24 14:22:09 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:22:25  31 of 43 OK created sql table model demo_gold.dim_customer ..................... [\u001b[32mOK\u001b[0m in 16.80s]\n",
      "\u001b[0m14:22:25  32 of 43 START sql table model demo_gold.dim_trade ............................. [RUN]\n",
      "25/01/24 14:22:25 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:25:37  32 of 43 OK created sql table model demo_gold.dim_trade ........................ [\u001b[32mOK\u001b[0m in 191.86s]\n",
      "\u001b[0m14:25:37  33 of 43 START sql table model demo_silver.trades .............................. [RUN]\n",
      "25/01/24 14:25:37 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:29:15  33 of 43 OK created sql table model demo_silver.trades ......................... [\u001b[32mOK\u001b[0m in 218.34s]\n",
      "\u001b[0m14:29:15  34 of 43 START sql table model demo_gold.dim_security .......................... [RUN]\n",
      "25/01/24 14:29:16 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:29:19  34 of 43 OK created sql table model demo_gold.dim_security ..................... [\u001b[32mOK\u001b[0m in 3.68s]\n",
      "\u001b[0m14:29:19  35 of 43 START sql table model demo_silver.watches_history ..................... [RUN]\n",
      "25/01/24 14:29:19 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:31:11  35 of 43 OK created sql table model demo_silver.watches_history ................ [\u001b[32mOK\u001b[0m in 111.60s]\n",
      "\u001b[0m14:31:11  36 of 43 START sql table model demo_gold.dim_account ........................... [RUN]\n",
      "25/01/24 14:31:11 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:31:21  36 of 43 OK created sql table model demo_gold.dim_account ...................... [\u001b[32mOK\u001b[0m in 10.21s]\n",
      "\u001b[0m14:31:21  37 of 43 START sql table model demo_silver.holdings_history .................... [RUN]\n",
      "25/01/24 14:31:21 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:32:52  37 of 43 OK created sql table model demo_silver.holdings_history ............... [\u001b[32mOK\u001b[0m in 91.41s]\n",
      "\u001b[0m14:32:52  38 of 43 START sql table model demo_silver.watches ............................. [RUN]\n",
      "25/01/24 14:32:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:35:23  38 of 43 OK created sql table model demo_silver.watches ........................ [\u001b[32mOK\u001b[0m in 150.92s]\n",
      "\u001b[0m14:35:23  39 of 43 START sql table model demo_gold.fact_cash_transactions ................ [RUN]\n",
      "25/01/24 14:35:23 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:37:34  39 of 43 OK created sql table model demo_gold.fact_cash_transactions ........... [\u001b[32mOK\u001b[0m in 130.53s]\n",
      "\u001b[0m14:37:34  40 of 43 START sql table model demo_gold.fact_trade ............................ [RUN]\n",
      "25/01/24 14:37:34 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m14:47:36  40 of 43 OK created sql table model demo_gold.fact_trade ....................... [\u001b[32mOK\u001b[0m in 602.28s]\n",
      "\u001b[0m14:47:36  41 of 43 START sql table model demo_gold.fact_holdings ......................... [RUN]\n",
      "25/01/24 14:47:36 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m15:05:26  41 of 43 OK created sql table model demo_gold.fact_holdings .................... [\u001b[32mOK\u001b[0m in 1069.51s]\n",
      "\u001b[0m15:05:26  42 of 43 START sql table model demo_gold.fact_watches .......................... [RUN]\n",
      "25/01/24 15:05:26 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m15:09:51  42 of 43 OK created sql table model demo_gold.fact_watches ..................... [\u001b[32mOK\u001b[0m in 265.33s]\n",
      "\u001b[0m15:09:51  43 of 43 START sql table model demo_gold.fact_cash_balances .................... [RUN]\n",
      "25/01/24 15:09:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "\u001b[0m15:12:47  43 of 43 OK created sql table model demo_gold.fact_cash_balances ............... [\u001b[32mOK\u001b[0m in 175.75s]\n",
      "\u001b[0m15:12:47  \n",
      "\u001b[0m15:12:47  Finished running 43 table models in 1 hours 25 minutes and 22.77 seconds (5122.77s).\n",
      "\u001b[0m15:12:47  \n",
      "\u001b[0m15:12:47  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m15:12:47  \n",
      "\u001b[0m15:12:47  Done. PASS=43 WARN=0 ERROR=0 SKIP=0 TOTAL=43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-84' coro=<ScriptMagics.shebang.<locals>._handle_stream() done, defined at /usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py:211> exception=ValueError('Separator is not found, and chunk exceed the limit')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 540, in readline\n",
      "    line = await self.readuntil(sep)\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 618, in readuntil\n",
      "    raise exceptions.LimitOverrunError(\n",
      "asyncio.exceptions.LimitOverrunError: Separator is not found, and chunk exceed the limit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\", line 213, in _handle_stream\n",
      "    line = (await stream.readline()).decode(\"utf8\", errors=\"replace\")\n",
      "  File \"/usr/lib/python3.8/asyncio/streams.py\", line 549, in readline\n",
      "    raise ValueError(e.args[0])\n",
      "ValueError: Separator is not found, and chunk exceed the limit\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt deps\n",
    "dbt run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae93304-68eb-46c8-86a3-26feec06a54a",
   "metadata": {},
   "source": [
    "## Run dbt tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b563f4-0b31-40a3-b353-9e6c9de8c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:15:37  Running with dbt=1.7.13\n",
      "\u001b[0m15:15:38  Registered adapter: spark=1.7.1\n",
      "\u001b[0m15:15:39  Found 44 models, 4 tests, 17 sources, 0 exposures, 0 metrics, 553 macros, 0 groups, 0 semantic models\n",
      "\u001b[0m15:15:39  \n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-47ebe326-bdfd-4b4e-ae5d-77717d286b16;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 667ms :: artifacts dl 26ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-47ebe326-bdfd-4b4e-ae5d-77717d286b16\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/26ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 15:15:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 15:15:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/01/24 15:15:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/24 15:15:55 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:15:55 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:15:55 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/01/24 15:15:55 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:15:55 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:16:22 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n",
      "\u001b[0m15:16:26  Concurrency: 1 threads (target='dev')\n",
      "\u001b[0m15:16:26  \n",
      "\u001b[0m15:16:26  1 of 4 START test fact_trade__unique_trade ..................................... [RUN]\n",
      "25/01/24 15:16:28 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "\u001b[0m15:16:55  1 of 4 PASS fact_trade__unique_trade ........................................... [\u001b[32mPASS\u001b[0m in 28.96s]\n",
      "\u001b[0m15:16:55  2 of 4 START test test_dates ................................................... [RUN]\n",
      "\u001b[0m15:17:07  2 of 4 PASS test_dates ......................................................... [\u001b[32mPASS\u001b[0m in 11.97s]\n",
      "\u001b[0m15:17:07  3 of 4 START test test_tax_values .............................................. [RUN]\n",
      "\u001b[0m15:17:22  3 of 4 PASS test_tax_values .................................................... [\u001b[32mPASS\u001b[0m in 15.02s]\n",
      "\u001b[0m15:17:22  4 of 4 START test test_week_day_names .......................................... [RUN]\n",
      "\u001b[0m15:17:24  4 of 4 PASS test_week_day_names ................................................ [\u001b[32mPASS\u001b[0m in 1.25s]\n",
      "\u001b[0m15:17:24  \n",
      "\u001b[0m15:17:24  Finished running 4 tests in 0 hours 1 minutes and 45.00 seconds (105.00s).\n",
      "\u001b[0m15:17:24  \n",
      "\u001b[0m15:17:24  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m15:17:24  \n",
      "\u001b[0m15:17:24  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $REPO_ROOT\n",
    "dbt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6ce00fe-ce46-4202-a798-a7f227ff3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-95fe46fb-7aa8-4a08-a5ba-6358290a1c26;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 434ms :: artifacts dl 43ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-95fe46fb-7aa8-4a08-a5ba-6358290a1c26\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/20ms)\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig (file:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar) to method sun.net.dns.ResolverConfiguration.open()\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.shaded.org.xbill.DNS.ResolverConfig\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 15:18:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 15:18:35 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n",
      "25/01/24 15:18:35 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/24 15:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/com.databricks_spark-xml_2.12-0.17.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/01/24 15:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/01/24 15:18:42 WARN Client: Same path resource file:///root/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TBD-TPC-DI-setup\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc5a6c54-50c6-42ad-a070-4e6e68ae3f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/24 15:20:19 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer bronze has 0 tables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer default has 0 tables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer demo_bronze has 17 tables.\n",
      "Layer demo_gold has 12 tables.\n",
      "Layer demo_silver has 14 tables.\n",
      "Layer digen has 17 tables.\n",
      "Layer gold has 0 tables.\n",
      "Layer silver has 0 tables.\n"
     ]
    }
   ],
   "source": [
    "   databases = spark.sql(\"show databases\").collect()\n",
    "   layers = [db.namespace for db in databases]\n",
    "   for layer in layers:\n",
    "      spark.sql(f\"use {layer}\")\n",
    "      table_count = spark.sql(\"show tables\").count()\n",
    "      print(f\"Layer {layer} has {table_count} tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c892605-9496-4526-b574-a19168678934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|     bronze|\n",
      "|    default|\n",
      "|demo_bronze|\n",
      "|  demo_gold|\n",
      "|demo_silver|\n",
      "|      digen|\n",
      "|       gold|\n",
      "|     silver|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2e94c83-4983-49f5-8597-77e3b9c4661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use demo_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44296edd-82e6-4600-b730-d2cc4992a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|demo_gold|         dim_account|      false|\n",
      "|demo_gold|          dim_broker|      false|\n",
      "|demo_gold|         dim_company|      false|\n",
      "|demo_gold|        dim_customer|      false|\n",
      "|demo_gold|            dim_date|      false|\n",
      "|demo_gold|        dim_security|      false|\n",
      "|demo_gold|           dim_trade|      false|\n",
      "|demo_gold|  fact_cash_balances|      false|\n",
      "|demo_gold|fact_cash_transac...|      false|\n",
      "|demo_gold|       fact_holdings|      false|\n",
      "|demo_gold|          fact_trade|      false|\n",
      "|demo_gold|        fact_watches|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e34fa44-084c-4445-ad41-17b4762a0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190b0c0-b4e2-4aed-85aa-1b78b77d1c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
